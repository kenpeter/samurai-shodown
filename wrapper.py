import gymnasium as gym
from gymnasium import spaces
from collections import deque
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from typing import Dict, Tuple, Optional, List
import math
import random


class JEPAStatePredictor(nn.Module):
    """
    JEPA-based opponent state prediction module
    Predicts opponent's next visual/combat state from current observations
    """

    def __init__(
        self, visual_dim=512, state_dim=64, sequence_length=8, prediction_horizon=4
    ):
        super().__init__()

        self.visual_dim = visual_dim
        self.state_dim = state_dim
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon

        # Visual context encoder for current game state
        self.context_encoder = nn.Sequential(
            nn.Linear(visual_dim, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
        )

        # Combat state extractor from visual features
        self.state_extractor = nn.Sequential(
            nn.Linear(visual_dim, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, state_dim),
            nn.Tanh(),  # Normalized state representation
        )

        # Temporal sequence encoder for opponent patterns
        self.temporal_encoder = nn.LSTM(
            input_size=128 + state_dim,  # context + previous state
            hidden_size=256,
            num_layers=2,
            batch_first=True,
            dropout=0.1,
        )

        # State prediction head (predicts opponent's next states)
        self.state_predictor = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, state_dim * prediction_horizon),  # Predict next N states
        )

        # Prediction confidence estimator
        self.confidence_head = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, prediction_horizon),  # Confidence for each prediction
            nn.Sigmoid(),
        )

        # Movement pattern classifier
        self.movement_classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(
                128, 8
            ),  # Attack, defend, advance, retreat, jump, crouch, idle, special
            nn.Softmax(dim=-1),
        )

        print(f"üîÆ JEPA State Predictor initialized:")
        print(f"   üìä Visual dim: {visual_dim}")
        print(f"   üéØ State dim: {state_dim}")
        print(f"   ‚è∞ Sequence length: {sequence_length}")
        print(f"   üîÆ Prediction horizon: {prediction_horizon}")

    def forward(self, visual_features, state_history):
        """
        Predict opponent's next states from current visual context

        Args:
            visual_features: (batch, visual_dim) - current visual features
            state_history: (batch, seq_len, state_dim) - historical opponent states

        Returns:
            predicted_states: (batch, horizon, state_dim)
            movement_probs: (batch, 8) - movement type probabilities
            confidence: (batch, horizon)
        """
        batch_size = visual_features.shape[0]

        # Encode current visual context
        context_encoded = self.context_encoder(visual_features)  # (batch, 128)

        # Extract current opponent state
        current_state = self.state_extractor(visual_features)  # (batch, state_dim)

        # Prepare sequence input: combine context with state history
        seq_len = state_history.shape[1]
        context_expanded = context_encoded.unsqueeze(1).expand(-1, seq_len, -1)
        sequence_input = torch.cat([context_expanded, state_history], dim=-1)

        # Temporal encoding
        lstm_out, (hidden, cell) = self.temporal_encoder(sequence_input)
        final_hidden = lstm_out[:, -1, :]  # Use last hidden state

        # Predict next states
        state_predictions = self.state_predictor(final_hidden)
        state_predictions = state_predictions.view(
            batch_size, self.prediction_horizon, self.state_dim
        )

        # Predict movement patterns
        movement_probs = self.movement_classifier(final_hidden)

        # Get confidence scores
        confidence = self.confidence_head(final_hidden)

        return state_predictions, movement_probs, confidence


class StrategicResponsePlanner(nn.Module):
    """
    Plans agent's response based on predicted opponent states
    Uses predicted opponent moves to select optimal agent actions
    """

    def __init__(
        self,
        visual_dim=512,
        opponent_state_dim=64,
        agent_action_dim=12,
        planning_horizon=4,
    ):
        super().__init__()

        self.visual_dim = visual_dim
        self.opponent_state_dim = opponent_state_dim
        self.agent_action_dim = agent_action_dim
        self.planning_horizon = planning_horizon

        # Current situation analyzer
        self.situation_analyzer = nn.Sequential(
            nn.Linear(visual_dim + opponent_state_dim, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
        )

        # Response strategy generator for each predicted opponent state
        self.response_generator = nn.Sequential(
            nn.Linear(
                128 + opponent_state_dim, 256
            ),  # situation + predicted opponent state
            nn.ReLU(inplace=True),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, agent_action_dim),  # Agent action logits
        )

        # Strategic value estimator
        self.value_estimator = nn.Sequential(
            nn.Linear(128 + opponent_state_dim + agent_action_dim, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 1),
            nn.Tanh(),
        )

        print(f"‚öîÔ∏è Strategic Response Planner initialized:")
        print(f"   üìä Visual dim: {visual_dim}")
        print(f"   üéØ Opponent state dim: {opponent_state_dim}")
        print(f"   üéÆ Agent action dim: {agent_action_dim}")
        print(f"   üìã Planning horizon: {planning_horizon}")

    def plan_response(
        self,
        visual_features,
        current_opponent_state,
        predicted_opponent_states,
        confidence_scores,
    ):
        """
        Plan agent's response sequence based on opponent predictions

        Args:
            visual_features: (batch, visual_dim)
            current_opponent_state: (batch, opponent_state_dim)
            predicted_opponent_states: (batch, horizon, opponent_state_dim)
            confidence_scores: (batch, horizon)

        Returns:
            response_actions: (batch, horizon, agent_action_dim)
            expected_values: (batch, horizon)
        """
        batch_size = visual_features.shape[0]

        # Analyze current situation
        situation_input = torch.cat([visual_features, current_opponent_state], dim=-1)
        situation_features = self.situation_analyzer(situation_input)

        response_actions = []
        expected_values = []

        for t in range(self.planning_horizon):
            if t < predicted_opponent_states.shape[1]:
                # Use predicted opponent state
                opp_state = predicted_opponent_states[
                    :, t, :
                ]  # (batch, opponent_state_dim)
                confidence = confidence_scores[:, t : t + 1]  # (batch, 1)

                # Generate response action
                response_input = torch.cat([situation_features, opp_state], dim=-1)
                response_logits = self.response_generator(response_input)

                # Weight by prediction confidence
                response_logits = (
                    response_logits * confidence
                )  # confidence is (batch, 1), broadcasts correctly

                response_actions.append(response_logits)

                # Evaluate expected value of this response
                response_probs = F.softmax(response_logits, dim=-1)
                value_input = torch.cat(
                    [situation_features, opp_state, response_probs], dim=-1
                )
                value = self.value_estimator(value_input)
                expected_values.append(
                    value.squeeze(-1)
                )  # Remove last dimension to make it (batch,)

            else:
                # Fallback for longer horizons
                fallback_input = torch.cat(
                    [situation_features, torch.zeros_like(current_opponent_state)],
                    dim=-1,
                )
                response_logits = self.response_generator(fallback_input)
                response_actions.append(response_logits)

                value = torch.zeros(
                    batch_size, device=visual_features.device
                )  # (batch,)
                expected_values.append(value)

        response_actions = torch.stack(
            response_actions, dim=1
        )  # (batch, horizon, agent_action_dim)
        expected_values = torch.stack(expected_values, dim=1)  # (batch, horizon)

        return response_actions, expected_values


class SamuraiJEPAWrapper(gym.Wrapper):
    """
    Enhanced Samurai Showdown wrapper with JEPA-based opponent state prediction
    and strategic agent response planning

    Features:
    - JEPA-style opponent state prediction from visual embeddings
    - Strategic response planning based on predictions
    - Enhanced reward system for predictive accuracy
    - Temporal pattern recognition for opponent behavior
    """

    def __init__(
        self,
        env,
        reset_round=True,
        rendering=False,
        max_episode_steps=15000,
        frame_stack=6,  # Changed to 6 frames
        frame_skip=4,
        target_size=(180, 126),
        enable_jepa=True,
        state_history_length=8,
    ):
        super().__init__(env)

        self.reset_round = reset_round
        self.rendering = rendering
        self.max_episode_steps = max_episode_steps
        self.frame_stack = frame_stack
        self.frame_skip = frame_skip
        self.target_size = target_size
        self.enable_jepa = enable_jepa
        self.prediction_horizon = frame_stack  # Prediction horizon = frame stack
        self.state_history_length = state_history_length

        # Health constants
        self.full_hp = 128

        # JEPA-specific components
        if self.enable_jepa:
            # Initialize JEPA modules (will be properly initialized after feature extractor is available)
            self.jepa_predictor = None
            self.response_planner = None
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

            # Opponent state tracking for JEPA
            self.opponent_state_history = deque(maxlen=state_history_length)
            self.predicted_opponent_states = None
            self.planned_agent_responses = None
            self.prediction_confidence = None
            self.movement_predictions = None

            # Initialize with dummy states
            dummy_state = np.zeros(64)  # 64-dim opponent state representation
            for _ in range(state_history_length):
                self.opponent_state_history.append(dummy_state.copy())

        # Enhanced tracking for strategic analysis
        self.strategic_stats = {
            "state_predictions_made": 0,
            "movement_prediction_accuracy": 0.0,
            "response_effectiveness": 0.0,
            "damage_after_prediction": 0.0,
            "successful_responses": 0,
            "total_responses": 0,
        }

        # Game state tracking
        self.prev_player_health = None
        self.prev_enemy_health = None
        self.prev_enemy_position = None
        self.prev_action = None
        self.step_count = 0
        self.episode_count = 0

        # Combat analysis
        self.combat_patterns = {
            "opponent_movement_sequences": [],
            "successful_predictions": [],
            "failed_predictions": [],
            "response_timings": [],
        }

        # Frame buffer for stacking
        self.frame_buffer = deque(maxlen=self.frame_stack)

        # Initialize frame buffer with zeros
        dummy_frame = np.zeros(
            (self.target_size[1], self.target_size[0], 3), dtype=np.uint8
        )
        for _ in range(self.frame_stack):
            self.frame_buffer.append(dummy_frame)

        # Update observation space for stacked RGB frames
        self.observation_space = spaces.Box(
            low=0,
            high=255,
            shape=(
                self.frame_stack * 3,
                self.target_size[0],
                self.target_size[1],
            ),
            dtype=np.uint8,
        )

        print(f"ü•∑ JEPA-ENHANCED SAMURAI SHOWDOWN WRAPPER:")
        print(f"   üìä Observation space: {self.observation_space.shape}")
        print(f"   üß† JEPA enabled: {self.enable_jepa}")
        if self.enable_jepa:
            print(
                f"   üîÆ Prediction horizon: {self.prediction_horizon} (auto: frame_stack)"
            )
            print(f"   üìö State history length: {self.state_history_length}")
            print(f"   ‚öîÔ∏è Strategic response planning: Enabled")
            print(f"   üéØ Visual state prediction: Enhanced")
        print(f"   üéÆ Frame stacking: {self.frame_stack} frames")
        print(f"   üéØ Target size: {self.target_size}")

    def _initialize_jepa_modules(self, visual_dim=512):
        """Initialize JEPA modules with proper dimensions"""
        if self.enable_jepa and self.jepa_predictor is None:
            self.jepa_predictor = JEPAStatePredictor(
                visual_dim=visual_dim,
                state_dim=64,
                sequence_length=self.state_history_length,
                prediction_horizon=self.prediction_horizon,
            ).to(self.device)

            self.response_planner = StrategicResponsePlanner(
                visual_dim=visual_dim,
                opponent_state_dim=64,
                agent_action_dim=self.env.action_space.n,
                planning_horizon=self.prediction_horizon,
            ).to(self.device)

            print(f"‚úÖ JEPA modules initialized with visual_dim={visual_dim}")

    def _preprocess_frame(self, frame):
        """Preprocess frame: resize and maintain RGB channels"""
        if len(frame.shape) == 3 and frame.shape[2] == 3:
            frame = cv2.resize(frame, self.target_size, interpolation=cv2.INTER_AREA)
        else:
            if len(frame.shape) == 2:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)
            frame = cv2.resize(frame, self.target_size, interpolation=cv2.INTER_AREA)
        return frame.astype(np.uint8)

    def _get_stacked_observation(self):
        """Create stacked observation from frame buffer"""
        stacked = np.concatenate(list(self.frame_buffer), axis=2)  # (H, W, 12)
        stacked = np.transpose(stacked, (2, 0, 1))  # (12, H, W)
        stacked = np.transpose(stacked, (0, 2, 1))  # (12, W, H)
        return stacked

    def _extract_game_state(self, info):
        """Extract enhanced game state information"""
        player_health = info.get("health", self.full_hp)
        opponent_health = info.get("enemy_health", self.full_hp)
        current_round = info.get("round", 1)
        score = info.get("score", 0)

        return {
            "player_health": player_health,
            "opponent_health": opponent_health,
            "current_round": current_round,
            "score": score,
        }

    def _extract_opponent_state_from_visuals(self, visual_features, game_info):
        """Extract opponent state representation from visual features and game info"""
        # This creates a 64-dimensional state representation of the opponent
        # combining health, position estimates, and visual pattern features

        opponent_health = game_info.get("opponent_health", self.full_hp)
        health_ratio = opponent_health / self.full_hp

        # Health-based features (8 dimensions)
        health_features = np.array(
            [
                health_ratio,
                1.0 - health_ratio,  # damage taken
                float(opponent_health > self.full_hp * 0.7),  # high health
                float(opponent_health < self.full_hp * 0.3),  # low health
                (
                    float(opponent_health > self.prev_enemy_health)
                    if self.prev_enemy_health
                    else 0
                ),  # gained health
                (
                    float(opponent_health < self.prev_enemy_health)
                    if self.prev_enemy_health
                    else 0
                ),  # lost health
                (
                    abs(opponent_health - self.prev_enemy_health) / self.full_hp
                    if self.prev_enemy_health
                    else 0
                ),  # health change rate
                float(opponent_health == 0),  # defeated
            ]
        )

        # Visual pattern features (simplified from actual visual features - 56 dimensions)
        # In practice, this would be learned features from the CNN
        if isinstance(visual_features, torch.Tensor):
            visual_features_np = visual_features.cpu().numpy()
            if visual_features_np.ndim > 1:
                visual_features_np = visual_features_np.flatten()
        else:
            visual_features_np = np.array(visual_features).flatten()

        # Reduce visual features to 56 dimensions using simple aggregation
        # In practice, you'd use learned dimensionality reduction
        if len(visual_features_np) >= 56:
            visual_compressed = visual_features_np[:56]
        else:
            visual_compressed = np.pad(
                visual_features_np, (0, 56 - len(visual_features_np)), "constant"
            )

        # Normalize visual features
        visual_compressed = np.tanh(visual_compressed * 0.1)  # Keep in reasonable range

        # Combine all features
        opponent_state = np.concatenate([health_features, visual_compressed])

        return opponent_state.astype(np.float32)

    def _predict_opponent_states(self, visual_features):
        """Use JEPA to predict opponent's next states"""
        if not self.enable_jepa or self.jepa_predictor is None:
            return None, None, None

        try:
            with torch.no_grad():
                # Prepare state history tensor
                state_history = torch.tensor(
                    np.array(list(self.opponent_state_history)),
                    dtype=torch.float32,
                    device=self.device,
                ).unsqueeze(
                    0
                )  # Add batch dimension

                # Predict opponent states
                predicted_states, movement_probs, confidence = self.jepa_predictor(
                    visual_features.unsqueeze(0), state_history
                )

                self.strategic_stats["state_predictions_made"] += 1

                return (
                    predicted_states.squeeze(0),
                    movement_probs.squeeze(0),
                    confidence.squeeze(0),
                )

        except Exception as e:
            print(f"JEPA state prediction error: {e}")
            return None, None, None

    def _plan_strategic_response(
        self, visual_features, current_opponent_state, predicted_states, confidence
    ):
        """Plan strategic agent response using predicted opponent states"""
        if (
            not self.enable_jepa
            or self.response_planner is None
            or predicted_states is None
        ):
            return None, None

        try:
            with torch.no_grad():
                # Ensure current_opponent_state is a tensor
                if isinstance(current_opponent_state, np.ndarray):
                    current_state_tensor = torch.tensor(
                        current_opponent_state, dtype=torch.float32, device=self.device
                    )
                else:
                    current_state_tensor = current_opponent_state

                # Ensure all tensors have batch dimension
                if visual_features.dim() == 1:
                    visual_features = visual_features.unsqueeze(0)
                if current_state_tensor.dim() == 1:
                    current_state_tensor = current_state_tensor.unsqueeze(0)
                if predicted_states.dim() == 2:
                    predicted_states = predicted_states.unsqueeze(0)
                if confidence.dim() == 1:
                    confidence = confidence.unsqueeze(0)

                response_actions, expected_values = self.response_planner.plan_response(
                    visual_features, current_state_tensor, predicted_states, confidence
                )

                return response_actions.squeeze(0), expected_values.squeeze(0)

        except Exception as e:
            print(f"Strategic response planning error: {e}")
            return None, None

    def _calculate_jepa_enhanced_reward(
        self, game_info, action, info, visual_features=None
    ):
        """Calculate enhanced reward including JEPA-based strategic bonuses"""
        # Base reward calculation (simplified version of original)
        base_reward = 0.0

        player_health = game_info["player_health"]
        enemy_health = game_info["opponent_health"]

        if self.prev_player_health is None:
            self.prev_player_health = player_health
            self.prev_enemy_health = enemy_health
            return 0.0

        # Health change rewards
        health_diff = self.prev_player_health - player_health
        enemy_health_diff = self.prev_enemy_health - enemy_health

        if enemy_health_diff > 0:
            base_reward += (enemy_health_diff / self.full_hp) * 5.0
        if health_diff > 0:
            base_reward -= (health_diff / self.full_hp) * 2.0

        # JEPA-enhanced strategic rewards
        jepa_bonus = 0.0

        if self.enable_jepa and visual_features is not None:
            # Movement prediction accuracy bonus
            if self.movement_predictions is not None and enemy_health_diff != 0:
                # Simple movement classification based on health change
                if enemy_health_diff > 0:
                    actual_movement = 0  # "attack" - enemy took damage
                elif health_diff > 0:
                    actual_movement = 1  # "defend" - player took damage
                else:
                    actual_movement = 6  # "idle" - no damage

                if len(self.movement_predictions) > actual_movement:
                    predicted_prob = self.movement_predictions[actual_movement].item()
                    accuracy_bonus = predicted_prob * 1.5
                    jepa_bonus += accuracy_bonus

                    # Update accuracy tracking
                    self.strategic_stats["movement_prediction_accuracy"] = (
                        self.strategic_stats["movement_prediction_accuracy"] * 0.9
                        + predicted_prob * 0.1
                    )

            # Strategic response effectiveness bonus
            if self.planned_agent_responses is not None and enemy_health_diff > 0:
                # Successful damage after strategic planning
                response_bonus = 2.0
                jepa_bonus += response_bonus

                self.strategic_stats["successful_responses"] += 1
                self.strategic_stats["damage_after_prediction"] += enemy_health_diff

            if self.planned_agent_responses is not None:
                self.strategic_stats["total_responses"] += 1

            # Prediction confidence reward
            if (
                self.prediction_confidence is not None
                and len(self.prediction_confidence) > 0
                and self.prediction_confidence[0] > 0.7
            ):  # High confidence prediction

                confidence_bonus = 0.5
                jepa_bonus += confidence_bonus

        # Update previous states
        self.prev_player_health = player_health
        self.prev_enemy_health = enemy_health

        return base_reward + jepa_bonus

    def reset(self, **kwargs):
        """Reset environment and JEPA tracking"""
        obs, info = self.env.reset(**kwargs)

        # Reset episode tracking
        self.step_count = 0
        self.episode_count += 1

        # Reset game state tracking
        self.prev_player_health = None
        self.prev_enemy_health = None
        self.prev_action = None

        # Reset JEPA tracking
        if self.enable_jepa:
            self.predicted_opponent_states = None
            self.planned_agent_responses = None
            self.prediction_confidence = None
            self.movement_predictions = None

            # Clear state histories
            dummy_state = np.zeros(64)
            self.opponent_state_history.clear()
            for _ in range(self.state_history_length):
                self.opponent_state_history.append(dummy_state.copy())

        # Reset frame buffer
        processed_frame = self._preprocess_frame(obs)
        self.frame_buffer.clear()
        for _ in range(self.frame_stack):
            self.frame_buffer.append(processed_frame)

        stacked_obs = self._get_stacked_observation()
        return stacked_obs, info

    def step(self, action):
        """Execute action with JEPA-enhanced strategic analysis"""
        total_reward = 0.0
        info = {}

        # Execute action with frame skipping
        for _ in range(self.frame_skip):
            obs, reward, terminated, truncated, step_info = self.env.step(action)
            total_reward += reward
            info.update(step_info)
            if terminated or truncated:
                break

        # Process frame and add to buffer
        processed_frame = self._preprocess_frame(obs)
        self.frame_buffer.append(processed_frame)

        # Get game information
        game_info = self._extract_game_state(info)

        # JEPA-enhanced processing
        visual_features = None
        if self.enable_jepa:
            # Generate mock visual features (in practice, this would come from the CNN)
            # This is a placeholder - in the full integration, this would be the actual
            # features from your JEPAEnhancedCNN
            visual_features = torch.randn(512, device=self.device)  # Mock features

            # Initialize JEPA modules if not done yet
            if self.jepa_predictor is None:
                self._initialize_jepa_modules(visual_dim=512)

            # Extract current opponent state from visuals and game info
            current_opponent_state = self._extract_opponent_state_from_visuals(
                visual_features, game_info
            )
            self.opponent_state_history.append(current_opponent_state)

            # Predict opponent's next states
            (
                self.predicted_opponent_states,
                self.movement_predictions,
                self.prediction_confidence,
            ) = self._predict_opponent_states(visual_features)

            # Plan strategic response
            if self.predicted_opponent_states is not None:
                response_plan = self._plan_strategic_response(
                    visual_features,
                    current_opponent_state,
                    self.predicted_opponent_states,
                    self.prediction_confidence,
                )
                if response_plan is not None:
                    self.planned_agent_responses, _ = response_plan

        # Calculate enhanced reward
        enhanced_reward = self._calculate_jepa_enhanced_reward(
            game_info, action, info, visual_features
        )

        # Episode termination logic
        done = (
            self.step_count >= self.max_episode_steps
            or terminated
            or truncated
            or (game_info["player_health"] <= 0 and game_info["opponent_health"] <= 0)
        )

        # Get stacked observation
        stacked_obs = self._get_stacked_observation()
        self.step_count += 1

        # Enhanced info for analysis
        info.update(
            {
                "game_info": game_info,
                "enhanced_reward": enhanced_reward,
                "jepa_enabled": self.enable_jepa,
                "strategic_stats": self.strategic_stats.copy(),
            }
        )

        if self.enable_jepa:
            info.update(
                {
                    "predicted_opponent_states": (
                        self.predicted_opponent_states.cpu().numpy()
                        if self.predicted_opponent_states is not None
                        else None
                    ),
                    "movement_predictions": (
                        self.movement_predictions.cpu().numpy()
                        if self.movement_predictions is not None
                        else None
                    ),
                    "prediction_confidence": (
                        self.prediction_confidence.cpu().numpy()
                        if self.prediction_confidence is not None
                        else None
                    ),
                    "planned_agent_responses": (
                        self.planned_agent_responses.cpu().numpy()
                        if self.planned_agent_responses is not None
                        else None
                    ),
                }
            )

        return stacked_obs, enhanced_reward, done, False, info

    def render(self, mode="human"):
        """Render environment with JEPA analysis overlay"""
        if self.rendering:
            return self.env.render()
        return None

    def close(self):
        """Close environment"""
        return self.env.close()


# Enhanced CNN Feature Extractor with JEPA integration
class JEPAEnhancedCNN(BaseFeaturesExtractor):
    """
    CNN Feature Extractor enhanced for JEPA integration
    Provides visual features suitable for opponent state prediction and strategic planning
    """

    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):
        super().__init__(observation_space, features_dim)

        n_input_channels = observation_space.shape[0]

        print(f"üß† Creating JEPA-Enhanced CNN...")

        # Main CNN backbone
        self.cnn = nn.Sequential(
            # First block - basic feature extraction
            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=2),
            nn.ReLU(inplace=True),
            # Second block - spatial patterns
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            # Third block - higher level features
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            # Fourth block - fighting game specific
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            # Global pooling
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
        )

        # Calculate CNN output size
        with torch.no_grad():
            sample = torch.zeros(1, n_input_channels, 126, 180)
            cnn_output_size = self.cnn(sample).shape[1]

        # Enhanced feature processing for JEPA
        self.feature_processor = nn.Sequential(
            nn.Linear(cnn_output_size, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(512, features_dim),
            nn.ReLU(inplace=True),
        )

        # Temporal feature enhancement for state prediction
        self.temporal_enhancer = nn.Sequential(
            nn.Linear(features_dim, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, features_dim),
            nn.Tanh(),  # Bounded output for stable temporal processing
        )

        # Visual state extraction branch (for opponent state representation)
        self.state_extractor = nn.Sequential(
            nn.Linear(features_dim, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 64),  # 64-dim opponent state
            nn.Tanh(),
        )

        print(f"üß† JEPA-Enhanced CNN:")
        print(f"   üìä Input: {observation_space.shape}")
        print(f"   üé® Input channels: {n_input_channels} (6-frame stack)")
        print(f"   üèóÔ∏è CNN output: {cnn_output_size}")
        print(f"   üéØ Final features: {features_dim}")
        print(f"   üîÆ JEPA-ready temporal features: Enabled")
        print(f"   üé≠ Opponent state extraction: 64-dim")

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        # Normalize input
        x = observations.float() / 255.0

        # CNN feature extraction
        cnn_features = self.cnn(x)

        # Process features
        processed_features = self.feature_processor(cnn_features)

        # Enhance for temporal processing (important for JEPA)
        temporal_features = self.temporal_enhancer(processed_features)

        # Combine original and temporal features
        final_features = processed_features + 0.3 * temporal_features

        return final_features

    def extract_opponent_state(self, observations: torch.Tensor) -> torch.Tensor:
        """Extract opponent state representation from visual observations"""
        # Get visual features
        visual_features = self.forward(observations)

        # Extract opponent state
        opponent_state = self.state_extractor(visual_features)

        return opponent_state


# Utility function to create JEPA-enhanced environment
def make_jepa_samurai_env(
    game="SamuraiShodown-Genesis",
    state=None,
    reset_round=True,
    rendering=False,
    max_episode_steps=15000,
    enable_jepa=True,
    frame_stack=6,  # Changed to 6 frames
    **wrapper_kwargs,
):
    """
    Create JEPA-enhanced Samurai Showdown environment with opponent state prediction
    Prediction horizon automatically matches frame_stack
    """
    import retro

    env = retro.make(
        game=game,
        state=state,
        use_restricted_actions=retro.Actions.FILTERED,
        obs_type=retro.Observations.IMAGE,
        render_mode="human" if rendering else None,
    )

    env = SamuraiJEPAWrapper(
        env,
        reset_round=reset_round,
        rendering=rendering,
        max_episode_steps=max_episode_steps,
        enable_jepa=enable_jepa,
        frame_stack=frame_stack,
        **wrapper_kwargs,
    )

    return env


if __name__ == "__main__":
    # Test the JEPA-enhanced wrapper
    print("üß™ Testing JEPA-Enhanced SamuraiShowdownWrapper...")

    try:
        env = make_jepa_samurai_env(rendering=False, enable_jepa=True)

        print(f"‚úÖ JEPA Environment created successfully")
        print(f"üìä Observation space: {env.observation_space}")
        print(f"üéÆ Action space: {env.action_space}")

        # Test reset
        obs, info = env.reset()
        print(f"üì∏ Reset observation shape: {obs.shape}")

        # Test steps with JEPA features
        for i in range(10):
            action = env.action_space.sample()
            obs, reward, done, truncated, info = env.step(action)

            jepa_info = ""
            if info.get("jepa_enabled"):
                pred_states = info.get("predicted_opponent_states")
                confidence = info.get("prediction_confidence")
                movement_preds = info.get("movement_predictions")

                if pred_states is not None and confidence is not None:
                    jepa_info = f", pred_conf={confidence[0]:.3f}"
                    if movement_preds is not None:
                        top_movement = np.argmax(movement_preds)
                        movement_names = [
                            "attack",
                            "defend",
                            "advance",
                            "retreat",
                            "jump",
                            "crouch",
                            "idle",
                            "special",
                        ]
                        jepa_info += f", movement={movement_names[top_movement]}"

            print(
                f"Step {i+1}: reward={reward:.3f}, enhanced={info.get('enhanced_reward', 0):.3f}{jepa_info}"
            )

            if done:
                break

        # Print JEPA stats
        if info.get("jepa_enabled"):
            stats = info.get("strategic_stats", {})
            print(f"üìä JEPA Strategic Stats:")
            for key, value in stats.items():
                if isinstance(value, float):
                    print(f"   {key}: {value:.3f}")
                else:
                    print(f"   {key}: {value}")

        env.close()
        print("‚úÖ JEPA-enhanced wrapper test completed successfully!")
        print("\nüéØ JEPA Capabilities Demonstrated:")
        print("   üîÆ Opponent state prediction from visual embeddings")
        print("   ‚öîÔ∏è Strategic agent response planning")
        print("   üé≠ Movement pattern classification")
        print("   üìä Confidence-weighted decision making")
        print("   üéÆ Enhanced reward system for strategic play")

    except Exception as e:
        print(f"‚ùå JEPA wrapper test failed: {e}")
        import traceback

        traceback.print_exc()
